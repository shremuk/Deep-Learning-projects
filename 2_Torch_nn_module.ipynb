{"cells":[{"cell_type":"markdown","metadata":{"id":"AXarCLfXYOIk","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# <font color = 'indianred'>**Lecture Goal**\n","In this lecture, we will understand PyTorch nn. Module. All the modules in Pytorch are implemented as subclass of the torch.nn.Module class. Pytorch uses these modules to perfrom operations on Tensors. We will first understand some importnat modules and then use these in implementing Linear Regression.\n","\n","We will first disuss following modules\n","\n","- nn.Linear()\n","- nn.Sequential()\n","- nn.MSELoss()\n","- torch.optim()\n","- torch.utils.data.DataLoader\n","- torch.utils.data.TensorDataset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M6mPJ9A0X6DX"},"source":["# <font color = 'indianred'>**Install Libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:12.016615Z","iopub.status.busy":"2022-09-25T18:21:12.016414Z","iopub.status.idle":"2022-09-25T18:21:12.019809Z","shell.execute_reply":"2022-09-25T18:21:12.019318Z","shell.execute_reply.started":"2022-09-25T18:21:12.016601Z"},"id":"u8KBBtPWUQHO","tags":[],"executionInfo":{"status":"ok","timestamp":1693197215297,"user_tz":300,"elapsed":8655,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# install torchviz libraries\n","if 'google.colab' in str(get_ipython()):\n","    !pip install torchsummary -qq"]},{"cell_type":"markdown","metadata":{"id":"4kxCqZPzbMQD"},"source":["# <font color = 'indianred'>**Import Libraries**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:03:51.543030Z","iopub.status.busy":"2022-09-25T18:03:51.542833Z","iopub.status.idle":"2022-09-25T18:03:51.884790Z","shell.execute_reply":"2022-09-25T18:03:51.884179Z","shell.execute_reply.started":"2022-09-25T18:03:51.543016Z"},"id":"q4ikzNObI7gs","executionInfo":{"status":"ok","timestamp":1693197225177,"user_tz":300,"elapsed":7054,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Importing PyTorch Library for building neural networks\n","import torch\n","\n","# Importing PyTorch's neural network module\n","import torch.nn as nn\n","\n","# Importing PyTorch's data loading utility\n","from torch.utils import data\n","\n","# Importing PyTorch's functional interface for neural network operations\n","import torch.nn.functional as F\n","\n","# Importing PyTorch's summary module for visualizing network architectures\n","import torchsummary\n","\n","# Importing the random library to generate random dataset\n","import random\n","\n","# Importing the math library for mathematical operations\n","import math"]},{"cell_type":"markdown","metadata":{"id":"V4WIZ1eOsLRN"},"source":["# <font color = 'indianred'>**nn.Module**\n","\n","`nn.Module` is a fundamental base class for all neural network modules in PyTorch, and it serves as a base class for defining your own neural network architectures. Subclassing `nn.Module` is crucial for creating a class that can hold your model's weights, biases, and other learnable parameters.\n","\n","By subclassing `nn.Module`, you gain access to a variety of helpful attributes and methods, including `.parameters()`. The `.parameters()` method returns an iterator over the model's parameters, which can be used for updating the weights during training.\n","\n","Subclassing `nn.Module` also makes it easier to work with pre-defined layers, loss functions, and other components that are provided by PyTorch. By organizing your model in this way, you can create a reusable and modular architecture that can be easily adapted to different tasks and datasets."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:04:47.959326Z","iopub.status.busy":"2022-09-25T18:04:47.959065Z","iopub.status.idle":"2022-09-25T18:04:47.962734Z","shell.execute_reply":"2022-09-25T18:04:47.962393Z","shell.execute_reply.started":"2022-09-25T18:04:47.959311Z"},"id":"HX_wx5JcuIVR","executionInfo":{"status":"ok","timestamp":1693197254011,"user_tz":300,"elapsed":135,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    \"\"\"\n","    A linear regression model that predicts a real-valued output based on a real-valued input.\n","    \"\"\"\n","\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Initializes the LinearRegression model.\n","\n","        Args:\n","            input_dim (int): The dimensionality of the input feature vector.\n","            output_dim (int): The dimensionality of the output feature vector.\n","        \"\"\"\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.weights = nn.Parameter(torch.randn(self.output_dim, self.input_dim) / math.sqrt(self.input_dim))\n","        self.biases = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Computes the forward pass of the LinearRegression model.\n","\n","        Args:\n","            x (torch.Tensor): The input feature vector.\n","\n","        Returns:\n","            torch.Tensor: The predicted output value.\n","        \"\"\"\n","        return x @ self.weights.T + self.biases\n"]},{"cell_type":"markdown","source":["- The `LinearRegression` class is a subclass of nn.Module in PyTorch and represents a linear regression model that predicts a real-valued output based on a real-valued input.\n","\n","- The `__init__` method is the constructor for the class, which takes two arguments: `input_dim` and `output_dim`. These arguments represent the dimensions of the input and output feature vectors, respectively. The method initializes the parameters for the linear regression model, including `weights` and `biases`, which are stored as `nn.Parameter` objects. The weights are initialized using a Gaussian distribution with a mean of 0 and a standard deviation of `1/sqrt(2)` to improve the stability of the training process.\n","\n","- The `forward` method computes the forward pass of the linear regression model, given an input feature vector x. The method returns the predicted output value, which is computed as the dot product of x and the model's weights, plus the model's biases. The weights and biases are learnable parameters of the model, which are optimized during the training process.\n","\n","- `nn.Module` objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call the forward method automatically.\n","\n","\n","Together, these methods define a linear regression model that can be used for a variety of regression tasks. By subclassing nn.Module, we gain access to a variety of helpful methods and attributes that make it easier to work with PyTorch's autograd system and perform backpropagation to update the model's parameters during training.\n"],"metadata":{"id":"eV_w94d0Dlcd"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:04:59.526725Z","iopub.status.busy":"2022-09-25T18:04:59.526239Z","iopub.status.idle":"2022-09-25T18:04:59.535177Z","shell.execute_reply":"2022-09-25T18:04:59.534690Z","shell.execute_reply.started":"2022-09-25T18:04:59.526709Z"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1693197260822,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"MdVgz3nnucF1","outputId":"06e87751-d0d0-4a9f-d614-26ce28aa490a"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([3, 2])\n","output_tensor shape:  torch.Size([3, 1])\n"]}],"source":["x = torch.arange(6).view(3, 2).float()\n","\n","# Input Dimension\n","input_dim = 2\n","\n","# Output Dimension\n","output_dim = 1\n","\n","# Since we're now using an object instead of just using a function, we\n","# first have to instantiate our model\n","\n","model = LinearRegression(input_dim, output_dim)\n","\n","# Get the output of linear layer after transformation\n","output = model(x)\n","\n","print('input_tensor shape :', x.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:05:03.747724Z","iopub.status.busy":"2022-09-25T18:05:03.747484Z","iopub.status.idle":"2022-09-25T18:05:03.751196Z","shell.execute_reply":"2022-09-25T18:05:03.750844Z","shell.execute_reply.started":"2022-09-25T18:05:03.747709Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693197263949,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"4bWveFdGwHW_","outputId":"a9d2fb06-a356-4177-b56e-5687eb9259ce"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[0.7943, 0.6915]], requires_grad=True)"]},"metadata":{},"execution_count":5}],"source":["model.weights"]},{"cell_type":"code","source":["model.biases"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DA-3oOlwFPpG","executionInfo":{"status":"ok","timestamp":1693197266537,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c3f19885-4b8f-480c-dbd9-6ee0fd1ca7c8"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([0.], requires_grad=True)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:05:04.690126Z","iopub.status.busy":"2022-09-25T18:05:04.689888Z","iopub.status.idle":"2022-09-25T18:05:04.693386Z","shell.execute_reply":"2022-09-25T18:05:04.692976Z","shell.execute_reply.started":"2022-09-25T18:05:04.690112Z"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1693197269894,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"fQvDV-IOwTIs","outputId":"7a9f082a-df07-4c8d-8f60-186e447c207e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x7defbcd53e60>"]},"metadata":{},"execution_count":7}],"source":["model.parameters()"]},{"cell_type":"code","source":["list(model.parameters())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JfamiIH3Cenu","executionInfo":{"status":"ok","timestamp":1693197278095,"user_tz":300,"elapsed":207,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"54219654-dad7-4981-9921-2dfb08f70371"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[0.7943, 0.6915]], requires_grad=True),\n"," Parameter containing:\n"," tensor([0.], requires_grad=True)]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["model.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd1NB-2BFBWD","executionInfo":{"status":"ok","timestamp":1693197282274,"user_tz":300,"elapsed":110,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"87707e08-fafc-400a-ae3b-a28151decfa7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weights', tensor([[0.7943, 0.6915]])),\n","             ('biases', tensor([0.]))])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"nHAZSWsYdxmB"},"source":["# <font color = 'indianred'>**Linear Module (nn.Linear)**\n"]},{"cell_type":"markdown","metadata":{"id":"es0C8VTzif1a"},"source":["Instead of manually defining and\n","initializing parameter (weights and biases), and calculating `x @ self.weights.T + self.biases`, we can use the Pytorch class `nn.Linear`for a\n","linear layer, which does all that for us.\n","\n","This layer takes in dimensions of input and output features and applies the following transformation to the input tensor $x$\n","\n","$y = x w^T + b$ ,\n","$w$ and $b$ are the parameters.\n","\n","The syntax for Linear Module is  :\n","`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n","\n","- in_features – size of each input sample\n","- out_features – size of each output sample\n","\n","Shapes :\n","\n","Input: $(N, *, H_{in})$ <br>\n","\n","here ,  $H_{in} = in\\_features$, ∗ means any number of additional dimensions and N is the batch size (number of observations). <br><br>\n","\n","Output: $(N ,*,  H_{out})$,\n","where all but the last dimension are the same shape as the input and $H_{out} = out\\_features$,\n","\n","\n","Example :\n","  - if input has shape(3, 2) (batch size is 3 and there are two features)\n","  and output = nn.Linear(in_features = 2, out_features =1)\n","  - then output will have the shape (3, 1) (3 observations and 1 feature).\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:07:47.055288Z","iopub.status.busy":"2022-09-25T18:07:47.055064Z","iopub.status.idle":"2022-09-25T18:07:47.058486Z","shell.execute_reply":"2022-09-25T18:07:47.057961Z","shell.execute_reply.started":"2022-09-25T18:07:47.055273Z"},"id":"J2irVf6S5PIJ","executionInfo":{"status":"ok","timestamp":1693197288644,"user_tz":300,"elapsed":118,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.linear_layer = nn.Linear(input_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        return self.linear_layer(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:31.878767Z","iopub.status.busy":"2022-09-25T18:11:31.878547Z","iopub.status.idle":"2022-09-25T18:11:31.882735Z","shell.execute_reply":"2022-09-25T18:11:31.882243Z","shell.execute_reply.started":"2022-09-25T18:11:31.878753Z"},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1693197309292,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"FkoRciKGifYr","outputId":"0bdf6c5e-8d46-4459-a4a4-631b0eeea119","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([3, 2])\n","output_tensor shape:  torch.Size([3, 1])\n"]}],"source":["x = torch.arange(6).view(3, 2).float()\n","\n","# Input Dimension\n","input_dim = 2\n","\n","# Output Dimension\n","output_dim = 1\n","\n","# Initialize first linear layer\n","model = LinearRegression(input_dim, output_dim)\n","\n","# Get the output of linear layer after transformation\n","output = model(x)\n","\n","print('input_tensor shape :', x.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"markdown","metadata":{"id":"9tla6bCKlXlu"},"source":["We have not specified any initial weights or bias values.  Linear module automatically initializes the weights randomly using LeCun initialization.  LeCun Initialization initializes weights using  $N(0, \\frac{1}{n_{in}})$\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:33.078018Z","iopub.status.busy":"2022-09-25T18:11:33.077816Z","iopub.status.idle":"2022-09-25T18:11:33.081225Z","shell.execute_reply":"2022-09-25T18:11:33.080815Z","shell.execute_reply.started":"2022-09-25T18:11:33.078005Z"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1693197319609,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"OuQZrL7IFdSg","outputId":"2eab4b22-87f9-4973-dbf4-aa9f291e810c","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["linear_layer.weight Parameter containing:\n","tensor([[0.1747, 0.1959]], requires_grad=True)\n","linear_layer.bias Parameter containing:\n","tensor([0.3106], requires_grad=True)\n"]}],"source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:11:33.964945Z","iopub.status.busy":"2022-09-25T18:11:33.964770Z","iopub.status.idle":"2022-09-25T18:11:33.968811Z","shell.execute_reply":"2022-09-25T18:11:33.968394Z","shell.execute_reply.started":"2022-09-25T18:11:33.964931Z"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1693197465900,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"VfiVv_fh-kSP","outputId":"c77acffb-2b02-4262-9a69-6c067cc87367","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["We can see that PyTorch initializes  weights  in the background\n","\n","W: Parameter containing:\n","tensor([[0.1747, 0.1959]], requires_grad=True)\n","b: Parameter containing:\n","tensor([0.3106], requires_grad=True)\n","Shape of W : torch.Size([1, 2])\n","Shape of b: torch.Size([1])\n"]}],"source":["print('We can see that PyTorch initializes  weights  in the background\\n')\n","print('W:', model.linear_layer.weight)\n","print('b:', model.linear_layer.bias)\n","print('Shape of W :', model.linear_layer.weight.data.shape)\n","print('Shape of b:', model.linear_layer.bias.data.shape)"]},{"cell_type":"markdown","metadata":{"id":"zxLN5mIU-3V-"},"source":["## <font color = 'indianred'>**Summary Linear Layer:**\n","\n","- When we initializes the layer (`layer = nn.Linear(input_dim, output_dim)`), Linear module takes the input and output dimensions as parameters, and automatically initializes the weights randomly.\n","\n","  - PyTorch sets the attribute requires_grad = True for weights and biases.\n","  - Shape of weights is [out_features, in_features]\n","  - Shape of bias is [out_features]\n","\n","- We can then apply this layer to inputs to get our output `(output = layer(input)`\n","  - It then uses randomly initilaized weights and biases to transform inputs.\n","\n","  - Shape of input = [batch_size, in_features]\n","  - output = input (W.T) + b\n","  - shape of output = [batch_size, out_features]\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1ewECT6hqC1sXd-TqXG3K1WZHAKXhY7g7\" width =700 >\n","\n","In the example above, the **output layer** would be `nn.Linear(2, 1)`. In the figure above, we have assumed a batch size of 1.\n"]},{"cell_type":"markdown","metadata":{"id":"UPv8zDkFBtKj"},"source":["# <font color = 'indianred'>**Shallow Neural Network**\n","\n","Many times, we want to compose Modules together. `torch.nn.Sequential` provides a good interface to combine modules sequentially where the output of a module (layer) is sequentially fed as an input to the next layer. Consider the following network:\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1rymZGH-Xrp_1ywGAcRJraiuuAd-verg7\" width =700 >\n","\n","\n","In the example above, the **hidden layer** would be `nn.Linear(3, 4)` and the **output layer** would be `nn.Linear(4, 1)`. In the figure above, we have assumed a batch size of 1."]},{"cell_type":"markdown","metadata":{"id":"P3phWDi25PIL"},"source":["## <font color = 'indianred'>**Shallow NN with Custom Class**"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:28.538529Z","iopub.status.busy":"2022-09-25T18:21:28.538366Z","iopub.status.idle":"2022-09-25T18:21:28.541726Z","shell.execute_reply":"2022-09-25T18:21:28.541260Z","shell.execute_reply.started":"2022-09-25T18:21:28.538516Z"},"tags":[],"id":"u_FVQDUv5PIL","executionInfo":{"status":"ok","timestamp":1693197795978,"user_tz":300,"elapsed":110,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.linear_layer1 = nn.Linear(input_dim,hidden_dim)\n","        self.linear_layer2 = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        out1 = self.linear_layer1(x)\n","        out2 = self.linear_layer2(out1)\n","        return out2"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T18:21:28.876204Z","iopub.status.busy":"2022-09-25T18:21:28.876033Z","iopub.status.idle":"2022-09-25T18:21:28.879789Z","shell.execute_reply":"2022-09-25T18:21:28.879347Z","shell.execute_reply.started":"2022-09-25T18:21:28.876191Z"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5OYVGBrp5PIM","executionInfo":{"status":"ok","timestamp":1693197797545,"user_tz":300,"elapsed":102,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"73270a17-4d0e-42b4-f582-f57701d274a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}],"source":["# The code below illustrates above eample with batch size of 5\n","input_ =   torch.arange(15).view(5, 3).float()\n","model = LinearRegression(3, 4 , 1)\n","output = model(input_)\n","\n","print('input_tensor shape :', input_.shape)\n","print('output_tensor shape: ', output.shape)"]},{"cell_type":"code","source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgnoOqFAc2T6","executionInfo":{"status":"ok","timestamp":1693197799258,"user_tz":300,"elapsed":102,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"73df8eb0-0b3c-4ac2-d372-2c2b6a6da0a9"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["linear_layer1.weight Parameter containing:\n","tensor([[ 0.5578, -0.1345,  0.3981],\n","        [-0.2833,  0.1780, -0.3086],\n","        [ 0.0540, -0.4669, -0.4543],\n","        [-0.4321, -0.4564,  0.5568]], requires_grad=True)\n","linear_layer1.bias Parameter containing:\n","tensor([ 0.0414,  0.2105, -0.4906, -0.4771], requires_grad=True)\n","linear_layer2.weight Parameter containing:\n","tensor([[ 0.0207, -0.1475, -0.2461, -0.1184]], requires_grad=True)\n","linear_layer2.bias Parameter containing:\n","tensor([-0.4169], requires_grad=True)\n"]}]},{"cell_type":"code","source":["model.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tp5C8trdc-0K","executionInfo":{"status":"ok","timestamp":1693197814943,"user_tz":300,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"45e5875a-1149-4530-dd22-afff8ecfc32d"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('linear_layer1.weight',\n","              tensor([[ 0.5578, -0.1345,  0.3981],\n","                      [-0.2833,  0.1780, -0.3086],\n","                      [ 0.0540, -0.4669, -0.4543],\n","                      [-0.4321, -0.4564,  0.5568]])),\n","             ('linear_layer1.bias',\n","              tensor([ 0.0414,  0.2105, -0.4906, -0.4771])),\n","             ('linear_layer2.weight',\n","              tensor([[ 0.0207, -0.1475, -0.2461, -0.1184]])),\n","             ('linear_layer2.bias', tensor([-0.4169]))])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## <font color = 'indianred'>**Shallow NN with nn.Sequential module**"],"metadata":{"id":"itMvAUuZWOYW"}},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:21:29.326177Z","iopub.status.busy":"2022-09-25T18:21:29.326003Z","iopub.status.idle":"2022-09-25T18:21:29.330135Z","shell.execute_reply":"2022-09-25T18:21:29.329735Z","shell.execute_reply.started":"2022-09-25T18:21:29.326163Z"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1693197828639,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"JyRZ0RjQBJ7R","outputId":"54cf36a3-b58e-4a7f-9bc3-18cc96818f67","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}],"source":["# The code below illustrates above eample with batch size of 5\n","input_ =   torch.arange(15).view(5, 3).float()\n","hidden_layer = nn.Linear(3, 4)\n","output_layer = nn.Linear(4, 1)\n","model = nn.Sequential(hidden_layer, output_layer)\n","output = model(input_)\n","\n","print('input_tensor shape :', input_.shape)\n","print('output_tensor shape: ', output.shape)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-09-25T18:21:29.713648Z","iopub.status.busy":"2022-09-25T18:21:29.713475Z","iopub.status.idle":"2022-09-25T18:21:29.716447Z","shell.execute_reply":"2022-09-25T18:21:29.716108Z","shell.execute_reply.started":"2022-09-25T18:21:29.713634Z"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1693197831113,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Csq8zhsFqXIO","outputId":"abcbcc42-50c5-4293-a67b-98dad46bcd96","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=3, out_features=4, bias=True)\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"]}],"source":["# print the model\n","print(model)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1693197832737,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"8Ty7WG2PxD3W","outputId":"606cb438-cc99-4833-be38-462cc83a65f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight Parameter containing:\n","tensor([[-0.5602, -0.5423,  0.3200],\n","        [-0.4775,  0.5487,  0.0946],\n","        [ 0.3972,  0.1628, -0.4469],\n","        [-0.5462,  0.5572,  0.2098]], requires_grad=True)\n","0.bias Parameter containing:\n","tensor([-0.5140,  0.3507, -0.2688,  0.0523], requires_grad=True)\n","1.weight Parameter containing:\n","tensor([[ 0.0076, -0.4039,  0.1838,  0.3969]], requires_grad=True)\n","1.bias Parameter containing:\n","tensor([-0.1348], requires_grad=True)\n"]}],"source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"]},{"cell_type":"code","source":["model.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s68-_VqKIneD","executionInfo":{"status":"ok","timestamp":1693197843339,"user_tz":300,"elapsed":244,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"2790d5d2-3a97-47c1-c684-0d6e3335b720"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight',\n","              tensor([[-0.5602, -0.5423,  0.3200],\n","                      [-0.4775,  0.5487,  0.0946],\n","                      [ 0.3972,  0.1628, -0.4469],\n","                      [-0.5462,  0.5572,  0.2098]])),\n","             ('0.bias', tensor([-0.5140,  0.3507, -0.2688,  0.0523])),\n","             ('1.weight', tensor([[ 0.0076, -0.4039,  0.1838,  0.3969]])),\n","             ('1.bias', tensor([-0.1348]))])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["## <font color = 'indianred'>**Sequential Module with Layer Names**"],"metadata":{"id":"tQqti42bWUzc"}},{"cell_type":"code","source":["# The code below illustrates above eample with batch size of 5\n","input_ =   torch.arange(15).view(5, 3).float()\n","model = nn.Sequential()\n","model.add_module('hidden_layer', nn.Linear(3, 4))\n","model.add_module('output_layer', nn.Linear(4, 1))\n","output = model(input_)\n","\n","print('input_tensor shape :', input_.shape)\n","print('output_tensor shape: ', output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qU9Twa6LIswa","executionInfo":{"status":"ok","timestamp":1693197847540,"user_tz":300,"elapsed":172,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a40ad544-e6f3-4003-995f-2e73412e1db8"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["input_tensor shape : torch.Size([5, 3])\n","output_tensor shape:  torch.Size([5, 1])\n"]}]},{"cell_type":"code","source":["# We can get all the parameters associated with model(linear layer) as follows\n","for name, param in  model.named_parameters():\n","  print(name, param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3CMetB-JG9-","executionInfo":{"status":"ok","timestamp":1693197849792,"user_tz":300,"elapsed":93,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"91afbecf-cf22-47ed-d24b-310e078c20df"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_layer.weight Parameter containing:\n","tensor([[-0.5254, -0.2667,  0.5607],\n","        [-0.4746,  0.1833, -0.1553],\n","        [ 0.4316, -0.5758,  0.1236],\n","        [ 0.0066, -0.0426,  0.1906]], requires_grad=True)\n","hidden_layer.bias Parameter containing:\n","tensor([-0.3567, -0.0695, -0.1711,  0.5599], requires_grad=True)\n","output_layer.weight Parameter containing:\n","tensor([[ 0.1396,  0.0535,  0.4285, -0.4434]], requires_grad=True)\n","output_layer.bias Parameter containing:\n","tensor([0.2394], requires_grad=True)\n"]}]},{"cell_type":"code","source":["print(model.state_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTNkqOuyJJ2J","executionInfo":{"status":"ok","timestamp":1693197852601,"user_tz":300,"elapsed":114,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"deb404ac-f0f0-4e7e-b850-e370512c71b0"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["OrderedDict([('hidden_layer.weight', tensor([[-0.5254, -0.2667,  0.5607],\n","        [-0.4746,  0.1833, -0.1553],\n","        [ 0.4316, -0.5758,  0.1236],\n","        [ 0.0066, -0.0426,  0.1906]])), ('hidden_layer.bias', tensor([-0.3567, -0.0695, -0.1711,  0.5599])), ('output_layer.weight', tensor([[ 0.1396,  0.0535,  0.4285, -0.4434]])), ('output_layer.bias', tensor([0.2394]))])\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5TQKmUksogN"},"source":["# <font color = 'indianred'>**Mean Squared Error Loss (nn.MSELoss())**\n","\n","PyTorch implements many common loss functions including `MSELoss` and `CrossEntropyLoss`. We will discuss `MSELoss()` in this lecture. We will explore `CrossEntropyLoss` in coming lectures.\n","\n","Supposedly our input and output is as follows:\n","\n","`x = [0, 1, 2, 3, 4]`\n","\n","`y = [1, 3, 5, 7, 9]`\n","\n","But our predicted output comes out with an error with equation `y = 2 * x`\n","\n","`ypred = [0, 2, 4, 6, 8] `\n","\n","Mean Squared Error (MSE) = $\\frac{\\sum_{i=1}^{n} (ypred_i  - y_i)^2} {n}$. Here, n = number of elements.\n","\n","For the above example, loss = 1.0\n","\n","Earlier we have written function to implement MSE. We can use nn.MSE() module from pytorch to calculate loss.\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1693197931617,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"CcuYiBAGvxtf","outputId":"5990a64c-29da-4a27-a2f7-1a44bdc05490"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.) tensor(1.) tensor(1.)\n"]}],"source":["# Instantiate Mean Squared Error loss function\n","def mse_loss(ypred, y):\n","    \"\"\"\n","    Computes the mean squared error loss between predicted and actual labels.\n","\n","    Args:\n","        ypred: a tensor of shape (num_examples, 1) containing the predicted labels\n","        y: a tensor of shape (num_examples, 1) containing the actual labels\n","\n","    Returns:\n","        A scalar tensor containing the mean squared error loss\n","    \"\"\"\n","    error = ypred - y\n","    mean_squared_error = torch.mean(error**2)\n","    return mean_squared_error\n","\n","# As a class - This is a higher order function that returns a function\n","loss_nn = nn.MSELoss(reduction='mean')\n","\n","\n","# As a function - This is a higher order function that retirns a function\n","loss_functional = F.mse_loss\n","\n","# when we specify reduction = 'mean' - this will give us mean sqaured loss\n","# if reduction = 'sum' - this will give us total squared loss\n","# reduction = 'mean' is the default\n","\n","# inputs\n","x = torch.Tensor([0, 1, 2, 3, 4])\n","y = torch.Tensor([1, 3, 5, 7, 9])\n","\n","# output\n","ypred = 2 * x\n","\n","# Calculating loss\n","# Loss function will take in 2 inputs: actual labels and predicted labels.\n","loss_manual = mse_loss(y, ypred)\n","loss_nn_module = loss_nn(y, ypred)\n","loss_functional = loss_functional(y, ypred)\n","print(loss_manual, loss_nn_module, loss_functional )"]},{"cell_type":"markdown","metadata":{"id":"NAcpc-BCPnKG"},"source":["# <font color = 'indianred'>**torch.optim**\n","We can implement number of gradient-based optimization methods using `torch.optim`. **SGD (Stochastic Gradient Descent)** is the most basic of them and **Adam** is one of the most popular. We will use SGD in this notebook and cover other optimizers in a later lecture.\n","\n","An optimizer takes the **model parameters** we want to update (learnable parameters), and the **learning rate**  (and some other hyper-parameters as well).\n","\n","Optimizers do not compute the gradients on their own, we need to call **backward()** on the loss first.\n","\n","We can then use optimizer's **step()** mehod to update the model parameters.\n","\n","Further, we do no not need to zero the gradients one by one. We can invoke the optimizer’s **zero_grad()** method.\n","\n","This does  `zero_()` call on all learnable parametets of the model."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1693197972318,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"pE0NfqSaW-eF","outputId":"497725cd-b6d3-4895-c64a-5cc04550e3ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["model params before weight update: tensor([[ 0.3865,  0.4989, -0.4232]]) tensor([-0.5237])\n","model params after weight update: tensor([[ 0.4332,  0.6389, -0.2365]]) tensor([-0.4770])\n"]}],"source":["# create a simple model\n","model = nn.Linear(3, 1)\n","\n","# create a simple dataset\n","X = torch.tensor([[1., 3., 4.]])\n","y = torch.tensor([[2.]])\n","\n","# create our optimizer\n","optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n","\n","# loss function\n","criterion = nn.MSELoss()\n","\n","y_hat = model(X)\n","\n","print('model params before weight update:', model.weight.data, model.bias.data)\n","\n","# calculate loss\n","loss = criterion(y_hat, y)\n","\n","# reset gradients to zero\n","optim.zero_grad()\n","\n","# calculate gradients\n","loss.backward()\n","\n","# update weights\n","optim.step()\n","\n","\n","print('model params after weight update:', model.weight.data, model.bias.data)\n"]},{"cell_type":"markdown","metadata":{"id":"Ra-i09LpZF5X"},"source":["# <font color = 'indianred'>**Dataset and Dataloader**\n","\n","When we train our model, we typically\n","\n","  - want to process the data in batches\n","  - reshuffle the data at every epoch to reduce model overfitting,\n","  - and use Python’s multiprocessing to speed up data retrieval.\n","\n","Earlier we wrote a function to create an iterator, that will shuffle the data and yield batches of data. However, we can do this much more efficently using **torch.utils.data.DataLoader**, which is an iterator that provides all the above features.\n","\n","The most important argument of DataLoader constructor is dataset, which is a PyTorch Dataset. Pytorch **Dataset** is a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class.\n","\n","If a dataset consists of tensors of lables and features, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to wrap tensors in a Dataset class.\n","\n","If the **dataset is big** (tens of thousands of image/text files, for instance), loading it at once would not be memory efficient. In that case we will need to create  custom dataset class , that load the files\\examples on demand. We will demonstrate how to create a CustomDataset that inherits from PyTorch's Dataset class later."]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1693197980519,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Rl2sL5gjbIEL","outputId":"595ff03f-0d6f-4f4b-dd88-985af1611d0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["x:tensor([[0., 1.],\n","        [2., 3.],\n","        [4., 5.],\n","        [6., 7.],\n","        [8., 9.]])\n","\n","y: tensor([[ 4.],\n","        [14.],\n","        [24.],\n","        [34.],\n","        [44.]])\n"]}],"source":["# Generate Dataset\n","x = torch.arange(10).view(5, 2)\n","x = x.type(dtype = torch.float)\n","w = torch.Tensor([2, 3]).view(-1, 1)\n","y = x.mm(w) + 1\n","print(f'x:{x}' )\n","print(f'\\ny: {y}')"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"Nm3CYK_2XwVN","executionInfo":{"status":"ok","timestamp":1693197982836,"user_tz":300,"elapsed":145,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Create Dataset\n","dataset = data.TensorDataset(x, y)"]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyL1FUysRodH","executionInfo":{"status":"ok","timestamp":1693197984166,"user_tz":300,"elapsed":198,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"85c77111-11a2-46cc-ace5-9af680d41e09"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataset.TensorDataset at 0x7df0825830d0>"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["dataset[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvlEmIuIRrS-","executionInfo":{"status":"ok","timestamp":1693197985429,"user_tz":300,"elapsed":119,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9cffdbb2-e0f4-450b-f023-1cc15d2d179e"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([2., 3.]), tensor([14.]))"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","execution_count":50,"metadata":{"id":"JxtqqOwVa3VN","executionInfo":{"status":"ok","timestamp":1693197987075,"user_tz":300,"elapsed":128,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Create DataLoader\n","data_iter = data.DataLoader(dataset, batch_size= 2, shuffle= True)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1693197988741,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"i5W6JDNubflG","outputId":"ff23854e-0052-4f92-de81-0c3cacb51e12"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1\n","\n","Batch Number 1\n","x:tensor([[2., 3.],\n","        [6., 7.]])\n","y: tensor([[14.],\n","        [34.]])\n","\n","Batch Number 2\n","x:tensor([[8., 9.],\n","        [0., 1.]])\n","y: tensor([[44.],\n","        [ 4.]])\n","\n","Batch Number 3\n","x:tensor([[4., 5.]])\n","y: tensor([[24.]])\n","\n","\n","Epoch 2\n","\n","Batch Number 1\n","x:tensor([[6., 7.],\n","        [4., 5.]])\n","y: tensor([[34.],\n","        [24.]])\n","\n","Batch Number 2\n","x:tensor([[8., 9.],\n","        [0., 1.]])\n","y: tensor([[44.],\n","        [ 4.]])\n","\n","Batch Number 3\n","x:tensor([[2., 3.]])\n","y: tensor([[14.]])\n","\n","\n","Epoch 3\n","\n","Batch Number 1\n","x:tensor([[0., 1.],\n","        [8., 9.]])\n","y: tensor([[ 4.],\n","        [44.]])\n","\n","Batch Number 2\n","x:tensor([[4., 5.],\n","        [2., 3.]])\n","y: tensor([[24.],\n","        [14.]])\n","\n","Batch Number 3\n","x:tensor([[6., 7.]])\n","y: tensor([[34.]])\n","\n"]}],"source":["# We can loop over the DataLoader object to get batch of observations\n","\n","for epoch in range(3):\n","  print(f'\\nEpoch {epoch + 1}\\n')\n","  for i, (x, y) in enumerate(data_iter):\n","    print(f'Batch Number {i+1}')\n","    print(f'x:{x}' )\n","    print(f'y: {y}\\n')"]},{"cell_type":"markdown","metadata":{"id":"lxO8HHxOlM9b"},"source":["We can obseve that in every epoch, an obsetvation is a part of a different batch. This happens as DataLoader shuffles the dataset to create batches."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}